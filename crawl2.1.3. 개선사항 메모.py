##  "collected_at": "2025-05-15 20:17:52" ë°ì´í„°ê°€ ë‚˜ì˜¤ì§€ ì•ŠëŠ” ìƒí™© ìˆ˜ì • ë°©ë²• ##
## ì‹œê°„ import ë° result í•¨ìˆ˜ êµ¬ì²´í™”í•˜ê¸°  ##

#ğŸ”§ ê¸°ì¡´ ì½”ë“œ ì˜ˆì‹œ (ë‹¨ìˆœí™”):

def process_post(post_url, driver, session):
    ...
    result = {
        "title": title,
        "author_id": author_id,
        "post_url": post_url,
        "files": extracted_files,
        ...
    }
    return result

# âœ…ìˆ˜ì •ëœ ì½”ë“œ ë‹¨ë½:

from datetime import datetime  # ğŸ“Œ ìƒë‹¨ import í•„ìš”

def process_post(post_url, driver, session):
    ...
    result = {
        "title": title,
        "author_id": author_id,
        "post_url": post_url,
        "files": extracted_files,
        ...
        "collected_at": datetime.now().strftime("%Y-%m-%d %H:%M:%S")  # ğŸ†• ì¶”ê°€
    }
    return result



### ì¶”ê°€ì‚¬í•­ ###

ğŸ“ ZIPSentinel 2.1.2 ì´í›„ Python ì½”ë“œ ìˆ˜ì • ë©”ëª¨ ëª©ë¡
âœ… 1. process_post()ì— "collected_at" í•„ë“œ ì¶”ê°€
ìœ„ì¹˜: result ë”•ì…”ë„ˆë¦¬ì— "collected_at" í•„ë“œ ì‚½ì…

í˜•ì‹: datetime.now().strftime("%Y-%m-%d %H:%M:%S")

ëª©ì : ê²Œì‹œê¸€ ë‹¨ìœ„ ìˆ˜ì§‘ ì‹œì  ê¸°ë¡

âœ… ë°˜ì˜ ì½”ë“œ:

python
ë³µì‚¬
í¸ì§‘
from datetime import datetime

result = {
    ...
    "collected_at": datetime.now().strftime("%Y-%m-%d %H:%M:%S")
}
âœ… 2. logging ì„¤ì •ì„ log.txt íŒŒì¼ì—ë„ ê¸°ë¡ë˜ë„ë¡ ë³€ê²½
ìœ„ì¹˜: logging.basicConfig() ì„¤ì •ë¶€

ë³€ê²½ ì´ìœ : Python ì‹¤í–‰ ë¡œê·¸ê°€ stdoutìœ¼ë¡œë§Œ ë‚˜ê°€ê³  ìˆì–´ì„œ íŒŒì¼ì— ì €ì¥ë˜ì§€ ì•ŠìŒ

âœ… ë°˜ì˜ ì½”ë“œ:

python
ë³µì‚¬
í¸ì§‘
import logging
import os

LOG_FILE = os.path.join(os.getcwd(), "log.txt")

logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s %(levelname)s: %(message)s",
    handlers=[
        logging.FileHandler(LOG_FILE, mode='a', encoding='utf-8'),
        logging.StreamHandler()
    ]
)
âœ… 3. (ì„ íƒ) ë¡œê·¸ ì €ì¥ ìœ„ì¹˜ ë³€ê²½ ì‹œ backup_watcher.shë„ ìˆ˜ì • í•„ìš”
log.txt ê²½ë¡œê°€ ë°”ë€Œë©´ ë°±ì—… ìŠ¤í¬ë¦½íŠ¸ì˜ ì´ ë¶€ë¶„ë„ í•¨ê»˜ ìˆ˜ì •:

bash
ë³µì‚¬
í¸ì§‘
docker cp crawler-2.1.2:/app/log.txt $TARGET_DIR/log.txt
ğŸ”§ ë¶€ê°€ ì‚¬í•­ (ë°˜ì˜ì€ ì„ íƒ)
í•­ëª©	ìƒíƒœ	ì„¤ëª…
save_result() êµ¬ì¡°	âœ… ë¬¸ì œ ì—†ìŒ	ì´ë¯¸ ë¦¬ìŠ¤íŠ¸ append êµ¬ì¡°ë¡œ ì˜ ì‘ì„±ë˜ì–´ ìˆìŒ
ëˆ„ë½ í•„ë“œ í™•ì¸	ğŸŸ¡ collected_at ì™¸ì—ëŠ” ì •ìƒ	í–¥í›„ post_id, category ë“± ë³´ê°• ê°€ëŠ¥

ğŸ“¦ ìš”ì•½: ìˆ˜ì • ëŒ€ìƒ íŒŒì¼ ëª©ë¡
íŒŒì¼ëª…	ìˆ˜ì • ë‚´ìš©
crawler2.1.2.py	process_post() + logging ì„¤ì •
backup_watcher.sh	(log.txt ê²½ë¡œ ë³€ê²½ ì‹œë§Œ) ë™ê¸°í™” í•„ìš”

